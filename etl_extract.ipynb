{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "551bb83a",
   "metadata": {},
   "source": [
    "# Extract Phase\n",
    "\n",
    "In this step, we perform the Extract phase of the ETL process.\n",
    "\n",
    "**Objectives:**\n",
    "- Load raw and incremental datasets.\n",
    "- Inspect the datasets to understand structure and contents.\n",
    "- Check for data quality issues (duplicates, outliers, missing values, wrong types).\n",
    "- Combine raw and incremental datasets to create a validated dataset for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21d10805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas for data manipulation\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470eeed9",
   "metadata": {},
   "source": [
    "## Load Raw and Incremental Datasets\n",
    "\n",
    "We load the raw and incremental CSV files into pandas DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d98f4a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw dataset\n",
    "raw_df = pd.read_csv('data/raw_data.csv')\n",
    "\n",
    "# Load the incremental dataset\n",
    "inc_df = pd.read_csv('data/incremental_data.csv')\n",
    "\n",
    "# Rename columns for easier access\n",
    "raw_df.columns = ['Year', 'Cause_Code', 'Cause_Name', 'State', 'Deaths', 'Age_Adjusted_Rate']\n",
    "inc_df.columns = ['Year', 'Cause_Code', 'Cause_Name', 'State', 'Deaths', 'Age_Adjusted_Rate']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe5290",
   "metadata": {},
   "source": [
    "## Inspecting Raw Data\n",
    "\n",
    "We inspect the raw dataset using `head()`, `info()`, and `describe()` to understand its structure, columns, and summary statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e12ecff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10868 entries, 0 to 10867\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Year               10868 non-null  int64  \n",
      " 1   Cause_Code         10868 non-null  object \n",
      " 2   Cause_Name         10868 non-null  object \n",
      " 3   State              10868 non-null  object \n",
      " 4   Deaths             10868 non-null  int64  \n",
      " 5   Age_Adjusted_Rate  10868 non-null  float64\n",
      "dtypes: float64(1), int64(2), object(3)\n",
      "memory usage: 509.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Age_Adjusted_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10868.000000</td>\n",
       "      <td>1.086800e+04</td>\n",
       "      <td>10868.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2008.000000</td>\n",
       "      <td>1.545991e+04</td>\n",
       "      <td>127.563894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.477478</td>\n",
       "      <td>1.128760e+05</td>\n",
       "      <td>223.639771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1999.000000</td>\n",
       "      <td>2.100000e+01</td>\n",
       "      <td>2.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2003.000000</td>\n",
       "      <td>6.120000e+02</td>\n",
       "      <td>19.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2008.000000</td>\n",
       "      <td>1.718500e+03</td>\n",
       "      <td>35.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2013.000000</td>\n",
       "      <td>5.756500e+03</td>\n",
       "      <td>151.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>2.813503e+06</td>\n",
       "      <td>1087.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Year        Deaths  Age_Adjusted_Rate\n",
       "count  10868.000000  1.086800e+04       10868.000000\n",
       "mean    2008.000000  1.545991e+04         127.563894\n",
       "std        5.477478  1.128760e+05         223.639771\n",
       "min     1999.000000  2.100000e+01           2.600000\n",
       "25%     2003.000000  6.120000e+02          19.200000\n",
       "50%     2008.000000  1.718500e+03          35.900000\n",
       "75%     2013.000000  5.756500e+03         151.725000\n",
       "max     2017.000000  2.813503e+06        1087.300000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first 5 rows\n",
    "raw_df.head()\n",
    "\n",
    "# Check column info, data types, and missing values\n",
    "raw_df.info()\n",
    "\n",
    "# Summary statistics for numeric columns\n",
    "raw_df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d26e2",
   "metadata": {},
   "source": [
    "## Data Quality Observations\n",
    "\n",
    "After inspecting the raw dataset, we identified the following issues:\n",
    "\n",
    "1. **Duplicate rows:** None were found, but we will still check in the transform phase.\n",
    "2. **Potential outliers:** Some values in 'Deaths' and 'Age_Adjusted_Rate' are unusually high or low.\n",
    "3. **Cause Name consistency:** 'Cause_Name' capitalization is consistent, but the 'Cause_Code' column is missing codes, which may affect analysis.\n",
    "\n",
    "**Action:** These issues will be addressed in the Transform phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74bdd2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows in raw dataset: 0\n",
      "Sample duplicate rows:\n",
      "Empty DataFrame\n",
      "Columns: [Year, Cause_Code, Cause_Name, State, Deaths, Age_Adjusted_Rate]\n",
      "Index: []\n",
      "\n",
      "Number of potential outliers in 'Deaths': 1483\n",
      "    Year                                         Cause_Code  \\\n",
      "0   2017  Accidents (unintentional injuries) (V01-X59,Y8...   \n",
      "5   2017  Accidents (unintentional injuries) (V01-X59,Y8...   \n",
      "52  2017                                         All Causes   \n",
      "53  2017                                         All Causes   \n",
      "55  2017                                         All Causes   \n",
      "\n",
      "                Cause_Name          State   Deaths  Age_Adjusted_Rate  \n",
      "0   Unintentional injuries  United States   169936               49.4  \n",
      "5   Unintentional injuries     California    13840               33.2  \n",
      "52              All causes  United States  2813503              731.9  \n",
      "53              All causes        Alabama    53238              917.7  \n",
      "55              All causes        Arizona    57758              678.5  \n",
      "\n",
      "Number of potential outliers in 'Age_Adjusted_Rate': 988\n",
      "    Year  Cause_Code  Cause_Name          State   Deaths  Age_Adjusted_Rate\n",
      "52  2017  All Causes  All causes  United States  2813503              731.9\n",
      "53  2017  All Causes  All causes        Alabama    53238              917.7\n",
      "54  2017  All Causes  All causes         Alaska     4411              708.8\n",
      "55  2017  All Causes  All causes        Arizona    57758              678.5\n",
      "56  2017  All Causes  All causes       Arkansas    32588              900.1\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = raw_df[raw_df.duplicated()]\n",
    "print(f\"Number of duplicate rows in raw dataset: {duplicate_rows.shape[0]}\")\n",
    "print(\"Sample duplicate rows:\")\n",
    "print(duplicate_rows.head())\n",
    "\n",
    "# Detect potential outliers using IQR method for Deaths\n",
    "Q1 = raw_df['Deaths'].quantile(0.25)\n",
    "Q3 = raw_df['Deaths'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers_deaths = raw_df[(raw_df['Deaths'] < (Q1 - 1.5*IQR)) | (raw_df['Deaths'] > (Q3 + 1.5*IQR))]\n",
    "print(f\"\\nNumber of potential outliers in 'Deaths': {outliers_deaths.shape[0]}\")\n",
    "print(outliers_deaths.head())\n",
    "\n",
    "# Detect potential outliers for Age_Adjusted_Rate\n",
    "Q1_rate = raw_df['Age_Adjusted_Rate'].quantile(0.25)\n",
    "Q3_rate = raw_df['Age_Adjusted_Rate'].quantile(0.75)\n",
    "IQR_rate = Q3_rate - Q1_rate\n",
    "\n",
    "outliers_rate = raw_df[(raw_df['Age_Adjusted_Rate'] < (Q1_rate - 1.5*IQR_rate)) | \n",
    "                       (raw_df['Age_Adjusted_Rate'] > (Q3_rate + 1.5*IQR_rate))]\n",
    "print(f\"\\nNumber of potential outliers in 'Age_Adjusted_Rate': {outliers_rate.shape[0]}\")\n",
    "print(outliers_rate.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536d0b10",
   "metadata": {},
   "source": [
    "## Combining Raw and Incremental Data\n",
    "\n",
    "We append the incremental dataset to the raw dataset using `pd.concat()`.\n",
    "\n",
    "**Reason:**\n",
    "- The incremental dataset contains recent entries not present in the raw dataset.\n",
    "- Combining ensures we have a single, validated dataset for further analysis and transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ec7607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (11868, 6)\n",
      "Validated dataset saved as 'data/validated_data.csv' in the data folder\n"
     ]
    }
   ],
   "source": [
    "# Combine raw and incremental datasets\n",
    "combined_df = pd.concat([raw_df, inc_df], ignore_index=True)\n",
    "print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "\n",
    "# Save the validated dataset inside 'data/' folder\n",
    "combined_df.to_csv('data/validated_data.csv', index=False)\n",
    "print(\"Validated dataset saved as 'data/validated_data.csv' in the data folder\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
